# step6_export_tflite.py

import tensorflow as tf
import numpy as np
import os
import sys
import traceback
import logging

from step3_model import build_ctc_transformer_model
from step2_dataset import load_char2idx
from step4_train import CTCModel


def export_tflite():
    try:
        # Táº¯t cáº£nh bÃ¡o log
        tf.get_logger().setLevel(logging.ERROR)
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

        print("ğŸ”§ Báº¯t Ä‘áº§u export mÃ´ hÃ¬nh sang TensorFlow Lite...")

        # Load vocab
        print("ğŸ“š Load vocabulary...")
        char2idx = load_char2idx()
        vocab_size = len(char2idx)
        print(f"âœ… Vocabulary size: {vocab_size}")

        # Khá»Ÿi táº¡o mÃ´ hÃ¬nh gá»‘c
        print("ğŸ§  Khá»Ÿi táº¡o mÃ´ hÃ¬nh...")
        base_model = build_ctc_transformer_model(
            input_dim=128,
            vocab_size=vocab_size,
            d_model=128,
            num_heads=4,
            num_layers=4,
            ff_dim=256,
        )

        # GÃ³i base_model vÃ o CTCModel Ä‘á»ƒ load Ä‘Ãºng weights
        ctc_model = CTCModel(base_model)

        # Build model báº±ng dummy input
        dummy_input = tf.random.uniform((1, 1000, 128), dtype=tf.float32)
        _ = ctc_model(dummy_input, training=False)

        # Load weights
        model_path = "checkpoints/best_model.weights.h5"
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file weights: {model_path}")
        ctc_model.load_weights(model_path)
        print(f"âœ… ÄÃ£ load weights tá»«: {model_path}")

        # Láº¥y láº¡i base_model Ä‘á»ƒ export
        trained_base_model = ctc_model.model
        inputs = trained_base_model.input
        logits = trained_base_model.output
        softmax_output = tf.keras.layers.Activation('softmax', name='softmax_output')(logits)
        export_model = tf.keras.Model(inputs=inputs, outputs=softmax_output)

        # Gá»i model vá»›i dummy input Ä‘á»ƒ khá»Ÿi táº¡o
        _ = export_model(dummy_input, training=False)

        # Táº¡o concrete function Ä‘á»ƒ export
        print("ğŸ”¨ Táº¡o concrete function Ä‘á»ƒ export...")
        @tf.function(input_signature=[tf.TensorSpec(shape=[1, 1000, 128], dtype=tf.float32)])
        def inference_fn(inputs):
            return export_model(inputs, training=False)

        concrete_func = inference_fn.get_concrete_function()

        # Convert sang TFLite
        print("ğŸ”„ Äang chuyá»ƒn Ä‘á»•i sang Ä‘á»‹nh dáº¡ng .tflite...")
        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
        converter.experimental_enable_resource_variables = False
        # converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Báº­t náº¿u cáº§n

        tflite_model = converter.convert()
        print("âœ… Chuyá»ƒn Ä‘á»•i thÃ nh cÃ´ng!")

        # LÆ°u model ra file
        os.makedirs("tflite_model", exist_ok=True)
        tflite_path = "tflite_model/model.tflite"
        with open(tflite_path, "wb") as f:
            f.write(tflite_model)
        print(f"ğŸ’¾ MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {tflite_path}")

        # Kiá»ƒm tra láº¡i mÃ´ hÃ¬nh TFLite
        interpreter = tf.lite.Interpreter(model_path=tflite_path)
        interpreter.allocate_tensors()
        print("ğŸ” Kiá»ƒm tra mÃ´ hÃ¬nh TFLite thÃ nh cÃ´ng.")

    except Exception:
        print("âŒ Gáº·p lá»—i trong quÃ¡ trÃ¬nh export:")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    export_tflite()
